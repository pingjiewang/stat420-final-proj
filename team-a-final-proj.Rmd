---
title: "Building an Used Car Price Prediction Model for Germany eBay listings"
author: "Richa Gupta/Sandro Tanis/Ping Wang"
date: "Aug 07, 2020"
output:
  html_document:
    theme: readable
    toc: yes
  word_document:
    toc: yes
  pdf_document: default
course: STAT 420, Summer 2020
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
library(knitr)
opts_chunk$set(cache = TRUE, autodep = TRUE)
knitr::opts_chunk$set(fig.width=10) 
```


# 1. Introduction

Why this Topic?

The formation of our group hinged upon our interest in the automotive industry, as a result of that we have chosen this robust dataset because we wanted to have a better understanding about the used car market and this dataset had all of the components that we wanted to observe for this project.The dataset that I am using in this project was found on Kaggle, the well-known Machine Learning Competition website and it is about Used Car listings from eBay â€“ Germany. 

The Data file contains approximately 370,000 observations and 20 variables that are scraped from used-car listing on Ebay-Kleinanzeigen (German). The high quantity and authenticity make this dataset useful for an exploratory analysis. Furthermore, the dataset is very large but what we have done to narrow it down is that we have decided to filter the data based on the year registration variable starting the year 2000 to now so we can build a good model based on the last 20 years in the used auto industry in Germany. Some of the of the variables that are very useful tous where price is used as a response to help build model predication when combined with other variables but not limited to : Power, Kilometer, VehicleType, YearRegistration as part of our additive model for this project as you will find out that based on our research we were able to determine whichmodel is sufficient for us to use based on these continuous variables in the dataset.

This project focuses on the exploratory data analysis phase of the dataset. In particular, we will try to detect associations between variables, especially against price. The end-goal of such a project would be to build a price-prediction model for vehicles sold by eBay users. This project broke down a lot of the pure concepts that allowed us to bring together the concepts we learned as well as provide background information to understand how these models are determined.

# 2. Methods

## 2.0 Data preparation

### 2.0.1 Loading necessary Libaries and the script file
```{r,warning=FALSE,message=FALSE}
library(dplyr)
library(readr)
library(lmtest)
library(MASS)
source("misc_functions.R")
```
The major libraries needed will be loaded from here to be able to be used throughout the document. This will include the libraries such as LMTEST to test linear regression model, read input from a csv files readr, and dplyr which enables us to manipulate the dataset.

### 2.0.2 Data loading
Loading the raw data from the CSV file:  
```{r,warning=FALSE,message=FALSE}
autos_raw <- read_csv("autos.csv")
attr(autos_raw, 'spec') <- NULL
attr(autos_raw, 'problems') <- NULL

str(autos_raw)
```


### 2.0.3 Data cleaning and unused columns removal

```{r}
autos=subset(autos_raw, price>500 & price<200000 & yearOfRegistration>=2000 & powerPS>0 & offerType=="Angebot" & seller=="privat")
columns_remove=c("postalCode","lastSeen","name","model","brand", "nrOfPictures","dateCreated","dateCrawled","monthOfRegistration","offerType","seller")   
columns_numeric = c("price","powerPS","yearOfRegistration","kilometer")
columns_factor = c("abtest","vehicleType","gearbox","fuelType","notRepairedDamage" )
autos=na.omit(autos)
autos = autos[, -which(names(autos) %in% columns_remove)]
```
Our data clearning process involved removing unuseful variable such as postalCode, lastSeen, name ect.. and minize to some of the useful variable that we could use which includes the numerical and factor variables.

After loading the raw data, **autos_raw** we: 

- Removed data with $price < 0$ 
- Keep only data with offerType="Angebot" and seller=="privat". **The data can either be in the form offertype = "Anagebot" and seller ="private"
- Keep only data with YearOfRegistration>=2000,  **We narrowed our dataset to include the last 20 years of used cars starting with the year 2000 and beyond
- Removed unused columns: `r columns_remove`,  **We removed unused/unwanted columns in our cleaning process
- Indentify continous numeric columns : `r columns_numeric`
- Indentify factor columns : `r columns_factor`
```{r}
#final data format
str(autos)
```


### 2.0.4 Checking colinearity on numberic variable
Taking a look at the continous variables only - checking for colinearity

```{r}
#Checking colinear on numberic columns on 50000 records sample
sample_size=50000
idx_sample=sample(1:nrow(autos),sample_size)
autos_sample= subset (autos[idx_sample,], select = columns_numeric) 
pairs(autos_sample,col="dodgerblue")
```
```{r}
round(cor(autos_sample), 2)
```
Based on our observation, we have determined there little to no collineararity issues wiht our model

### 2.0.5 Setup training and testing data

```{r}
#Setup data with 10000 randomly sampled and all columns
training_size=50000
set.seed(20200807)
idx_train=sample(1:nrow(autos),training_size)
autos_train= autos[idx_train,]
autos_train[,columns_factor]=lapply(autos_train[,columns_factor], as.factor)
str(autos_train)

#test data
test_size=training_size
idx_remain = !(1:nrow(autos) %in% idx_train)
autos_remain = autos[idx_remain, ]
idx_test=sample(1:nrow(autos_remain),test_size)
autos_test= autos_remain[idx_test,]
autos_test[,columns_factor]=lapply(autos_test[,columns_factor], as.factor)
rm(autos_remain)
```

## 2.1 Method 1 - a straightforward but dump approach


## 2.1.1 Start with an additive model using all predictors
```{r}

model1_start=lm(price~.,data=autos_train)
#size of staring model
length( coef(model1_start) )

```

## 2.1.2 Run BIC backward search
```{r}
#Run BIC backward search
n=nrow(autos_train)
model1_bic = step(model1_start,k=log(n),trace = 0)
#size of selected model by backward BIC
length( coef(model1_bic) )
model1_bic
```

## 2.1.3 Check for violation of model assumptions
```{r}
diagnostics(model1_bic)
model1_selected=model1_bic
```


## 2.1.4 See if the assumption violations are improved when inflential points are removed

```{r}
remove_high_influential_points_and_refit_model(model1_selected,autos_train)
```

for the following method we pick a data sample_size of 50000 and with fit into an additive model with all of the variables to display the lenght of coef then we ran backward BIC to remove fsome of the predictors then we have a source where we place our functions to pull our model assumption to find out if our model is credible. Based on the results, we have concluded this is not a good model. Fitted Residuals and Q-Q Plots indicated some sort variable tranformations are needed.

## 2.2  Method 2 - With variable transformations

We will take a 2-phases approach to finding a good model:

- Phase I - the Continuous variables phase 
- Pahse II - Adding categorical variables the 


### 2.2.1 - Phase 1: working with continous variable to determine the best model form for continous columns

#### 2.2.1.1 - break data-set into subgroups by fixing factor variable values

```{r}
#listing out all factor columns
columns_factor

autos_factor_groups=autos %>% count (abtest,vehicleType,gearbox,fuelType,notRepairedDamage)
autos_factor_groups=autos_factor_groups[order(autos_factor_groups$n,decreasing = TRUE),]

#structure of autos_factor_groups
head(autos_factor_groups)

#Total number of groups
nrow(autos_factor_groups)

#Number of groups with more than 300 records
sum(autos_factor_groups$n>300)
```
Our data exploration continued though a process of breaking our dataset into subgroups by categorizing with the following variables: abtest, vehicleType, gearbox, model, fuelType and brand. For example abtest can only have either be test or control, and vehicleType can either be Limousine or Kleinwagon, we have picked a number of groups with more than 300 records in order to find a definitive model for our dataset. 

#### 2.2.1.2 - work throght one subgroup first. (choose group 2)
```{r}
#choose group
selected_group_idx=2
group1=autos_factor_groups[selected_group_idx, ]
(group1.size = group1$n)
group1=subset(group1, select = -c(n) )

#get the records for the selected group
autos_1=autos
cols=names(group1)
for (i in 1:ncol(group1)){
  idx = autos_1[,cols[i]]==group1[[i]]
  autos_1=autos_1[idx,]
}
autos_1=subset(autos_1, select = columns_numeric )

```

todo: pairs(autos_1,col="dodgerblue"), and explain the reason we isolated the group

We will use the newly isolated dataset, **autos_1** to better help finding a model form for continuous variables

#### 2.2.1.3 - Try an additive model using all continous variables

```{r}
par(mfrow=c(1,2))
model2_add = lm(price ~ powerPS +yearOfRegistration+kilometer, data = autos_1)
diagnostics(model2_add)
```

The plots and test pValue indicates signifant voiloation of equal variance and normality assumption. We will try Box-Cox transformation on Price next.


#### 2.2.1.4 - Try Box-Cox tranformation on the selected group


```{r}
par(mfrow=c(1,1))
boxcox_input_formula= as.formula ( as.character(model2_add$call[2]) )
out=boxcox(model2_add, plotit = TRUE, lambda = seq(-0.5, 1.0, by = 0.05))
( lambda=out$x[which.max(out$y)] )
model2_add_cox = lm( ((price^lambda-1)/lambda) ~ powerPS +yearOfRegistration , data = autos_1 )
diagnostics(model2_add_cox)
```

Plots looks better but it seems additional predictor transformation might help.

#### 2.2.1.5 - finding predictor tranformation via backward BIC search

```{r}
# starting with log and 2nd order terms for all predictors
model2a_bic_start=lm( (price^lambda-1)/lambda ~ powerPS+I(log(powerPS)) + I(powerPS^2)
                        +yearOfRegistration + I(log(yearOfRegistration)) + I(yearOfRegistration^2) 
                        +kilometer + I(log(kilometer)) + I(kilometer^2)
                      , data = autos_1 )
  



model2a_bic = step(model2a_bic_start,trace=0)
diagnostics(model2a_bic)
summary(model2a_bic)

#Save model2a for later steps
model2a=model2a_bic
formula_str=as.character(model2a$call[2])
```

The formula form of model(using only continuous variables) is:

- **`r formula_str`**


### 2.2.2 - Phase 2: adding categorical variables to the model that was determined in Phase 1

The best model form with only continous variable is:  

- `r formula_str`  
- Where we determine the new $\lambda$ value based on entire training dataset

Next, we will add all factor varibles to the formula format we obtained from the Phase 1 as the starting model in backward BIC search.

#### 2.2.2.0 - Run box-cox tranformation on all training data
We will determine new $\lambda$ value by fitting established model form on all training data

- **Formula format determined from Phase 1**: `r formula_str`
  
```{r}

par(mfrow=c(1,1))

model_2a_all= lm (price ~ powerPS + I(log(powerPS)) + I(powerPS^2) + I(log(yearOfRegistration)) + kilometer + I(log(kilometer)),data=autos_train)

diagnostics(model_2a_all)

par(mfrow=c(1,1))
out=boxcox(model_2a_all, plotit = TRUE, lambda = seq(-0.5, 1.0, by = 0.05))
( lambda=out$x[which.max(out$y)] )


```
  
  
#### 2.2.2.1 Searching for a good model using backward BIC   
```{r}
par(mfrow=c(1,1))
#model2_start = lm( ((price^lambda - 1)/lambda) ~ .-vehicleType +  I(powerPS^2), data=autos_train ) 
model2_start = lm( ((price^lambda - 1)/lambda) ~ .+I(log(powerPS)) + I(powerPS^2) + I(log(yearOfRegistration)) + I(log(kilometer)), data=autos_train ) 

n=nrow(autos_train)
model2_selected_bic = step(model2_start,k=log(n),trace=0)
summary(model2_selected_bic)

diagnostics(model2_selected_bic)

model2_selected=model2_selected_bic

model2_selected_bic$call[2]
```


#### 2.2.2.2 verify regression signifcant with ANOVA test on high p-Value betas(vehicleType and fuelType)

-vehicleType

```{r}
null_model_str = paste (as.character(model2_selected_bic$call[2]), "-vehicleType")
model_null= lm(null_model_str, data = autos_train)
anova(model_null,model2_selected_bic)
```

-fuelType

```{r}
null_model_str = paste (as.character(model2_selected_bic$call[2]), "-fuelType")
model_null= lm(null_model_str, data = autos_train)
anova(model_null,model2_selected_bic)
```


####  2.2.2.3 Remove high influence data and refit the Phase 2 model

```{r}
fix_model=remove_high_influential_points_and_refit_model(model2_selected_bic, autos_train)
fix_model

```

## 2.3  Method 3 - find an interaction model based on method 2 (optional)
We will do this if we have time

```{r,eval=FALSE}
library(tictoc)
lambda
model3a_bic_start=lm( (price^lambda-1)/lambda ~ .^2
                      + I(log(powerPS)) + I(powerPS^2)
                      + I(log(yearOfRegistration)) + I(yearOfRegistration^2) 
                      + I(log(kilometer)) + I(kilometer^2)
                      , data = autos_1 )

model3a_bic = step(model2a_bic_start,trace=0)
diagnostics(model3a_bic)
summary(model3a_bic)

```


# 3. Results
```{r}
# function to evaluate rmse
rmse  = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}

#mod1 RMSEs
Train_RMSE_mod1 = rmse(autos_train$price, predict(model1_selected, autos_train, type = 'response'))
Test_RMSE_mod1 =  rmse(autos_test$price, predict(model1_selected, autos_test,type = 'response'))

#mod2 RMSEs
Train_RMSE_mod2 = rmse(autos_train$price, (predict(model2_selected, newdata = autos_train, type ='response')*lambda+1)^(1/lambda))
Test_RMSE_mod2 = rmse(autos_test$price, (predict(model2_selected, newdata = autos_test, type ='response')*lambda+1)^(1/lambda))

# calculate all train errors
train_error = c(Train_RMSE_mod1, Train_RMSE_mod2)

# calculate all test errors
test_error = c(Test_RMSE_mod1, Test_RMSE_mod2)

auto_models = c("`Additive model`", "`Transformation model`")
auto_results = data.frame(auto_models, train_error, test_error)
colnames(auto_results) = c("Model", "Train RMSE", "Test RMSE")
knitr::kable(auto_results)
```

# 4. Discussion

# 5. Appendix

#### The listing for R code used

```{r engine='bash', comment=''}
cat -n misc_functions.R
```




