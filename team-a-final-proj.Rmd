---
title: 'Building a price prediction model for eBay Germany Used Car listings'
author: "Richa Gupta/Sandro Tanis/Ping Wang"
course: "STAT 420, Summer 2020"
date: 'Aug 07, 2020'
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
library(knitr)
opts_chunk$set(cache = TRUE, autodep = TRUE)
```

# 1. Introduction

# 2. Methods

## 2.1 Data cleanning and exploration

Loading for raw data
```{r,warning=FALSE,message=FALSE}
library(readr)
autos_raw <- read_csv("autos.csv")
attr(autos_raw, 'spec') <- NULL
attr(autos_raw, 'problems') <- NULL

str(autos_raw)
```


## 2.1.1 Data cleaning and preparation

```{r}
autos=subset(autos_raw, price>0 & yearOfRegistration>=2000 & offerType=="Angebot" & seller=="privat")
columns_remove=c("postalCode","lastSeen", "nrOfPictures","dateCreated","dateCrawled","monthOfRegistration","offerType","seller")   
columns_numeric = c("price","powerPS", "kilometer","yearOfRegistration")
columns_factor = c("abtest","vehicleType","gearbox","model","brand","fuelType","notRepairedDamage" )
other_columns = c("name") 

#autos[,columns_factor]=lapply(autos[,columns_factor], as.factor)
autos = autos[, -which(names(autos) %in% columns_remove)]
autos = autos[, -which(names(autos) %in% other_columns)]
str(autos)


```




## 2.1.2 Data exploration to find the relationship between continous variables

```{r}
library(dplyr)
autos=na.omit(autos)
autos_factor_groups=autos %>% count (abtest,vehicleType,gearbox,model,brand,fuelType,notRepairedDamage)
autos_factor_groups=autos_factor_groups[order(autos_factor_groups$n,decreasing = TRUE),]
group1=autos_factor_groups[2,]
group1.size = group1$n
group1=subset(group1, select = -c(n) )

autos_1=autos
cols=names(group1)
for (i in 1:ncol(group1)){
  idx = autos_1[,cols[i]]==group1[[i]]
  autos_1=autos_1[idx,]
}

autos_1=subset(autos_1, select = columns_numeric )
  

#
pairs(autos_1,col="dodgerblue")
```

### Autos_1 has been isolated to better find a model format for continuous variables

Please use **autos_1** for determining box-cox tranformations,it only contains continuous variables 

```{r}
par(mfrow=c(1,2))
model_add = lm(price ~ powerPS + kilometer+yearOfRegistration , data = autos_1)
#fitted vs residual
plot(fitted(model_add), resid(model_add), col = "dodgerblue", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals of additive")
abline(h = 0, col = "darkorange", lwd = 2)

#qqplot
qqnorm(resid(model_add), main = "Normal Q-Q Plot of additive", col = "dodgerblue")
qqline(resid(model_add), col = "dodgerblue", lwd = 2)

```

## 2.1.3 perform box-cox tranformation on continuous variables only (to estabish transformation form )
```{r}

library(MASS)
library(lmtest)
par(mfrow=c(1,1))
out=boxcox(model_add, plotit = TRUE, lambda = seq(-0.5, 2.0, by = 0.1))
( lambda=out$x[which.max(out$y)] )

model_cox_start=lm( (price^lambda-1)/lambda ~ powerPS + kilometer+yearOfRegistration+I(powerPS^2) + I(kilometer^2) + I(log(kilometer)), data = autos_1)
model_cox = step(model_cox_start,trace=0)
#model_cox
#model_cox2 = step(model_cox_start,k=log(nrow(autos_1)),trace=0)
#model_cox2

#model_add_cox=lm((price^lambda-1)/lambda ~ powerPS, data = autos_1)

summary(model_cox)

par(mfrow=c(1,2))
plot(fitted(model_cox), resid(model_cox), col = "dodgerblue", 
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals with Box-cox")
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(model_cox), main = "Normal Q-Q Plot with Box-cox", col = "dodgerblue")
qqline(resid(model_cox), col = "dodgerblue", lwd = 2)

shapiro.test(resid(model_cox))
bptest(model_cox)


```

### tempory block - refit the multiple regression model without any influential points

```{r}
#finding influenctial
mod_cook = cooks.distance(model_cox)
n=length(resid(model_cox))
high_infl = mod_cook > 4 / n
sum(high_infl)     
mean(high_infl)


#Refit the multiple regression model without any influential points
formula=as.formula(as.character(model_cox$call[2]))
model_cox_sub = lm(formula, data = autos_1, subset = !high_infl)
par(mfrow=c(1,2))
plot(fitted(model_cox_sub), resid(model_cox_sub), col = "dodgerblue", 
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals with Box-cox")
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(model_cox_sub), main = "Normal Q-Q Plot with Box-cox", col = "dodgerblue")
qqline(resid(model_cox_sub), col = "dodgerblue", lwd = 2)

summary(model_cox_sub)

#(coef(int_model) - coef(int_model_sub)) / coef(int_model)
shapiro.test(resid(model_cox_sub))
bptest(model_cox_sub)

```

## 2.1.4 determine best lambda for box-cox tranformation 

We will run box-cox transformation on subgroup of data that has more than 300 records..

```{r, eval=FALSE}
source("misc_functions.R")
(boxcox_lambda=subset_autodata_with_boxcox(autos))
hist(boxcox_lambda,breaks=20,col="lightblue")
mean(boxcox_lambda)
```
Based on our analysis above, we will use $\lambda=0.3$ for the Box-Cox transformation!

```{r}
lambda=0.3
formula_str=as.character(model_cox$call[2])
```


### Phase II - adding categorical variable to the model form determine in Phase I

##### The best model form with only continous variable is:
`r formula_str`

#### Next, we combine continous variables(wiht established form) with all factor varible to start a backward AIC to find a good model.
  
```{r}
size_train=2000

idx_train=sample(1:nrow(autos),size_train)
autos_train=autos[idx_train,]
autos_train[,columns_factor]=lapply(autos_train[,columns_factor], as.factor)
#model2_start = lm( ((price^lambda - 1)/lambda) ~ .^2 +  I(powerPS^2) + I(kilometer^2) + I(log(kilometer)), autos_train ) 
model2_start = lm( ((price^lambda - 1)/lambda) ~ . +  I(powerPS^2) + I(kilometer^2) + I(log(kilometer)), data=autos_train ) 
n=nrow(autos_train)
model2_selected_bic = step(model2_start,k=log(n),trace=0)
model2_selected_bic
```

##  2.1.4 remove high influence data and refit the choose model

```{r}
model =model2_selected_bic
par(mfrow=c(1,2))
plot(fitted(model), resid(model), col = "dodgerblue", 
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(model), main = "Normal Q-Q Plot", col = "dodgerblue")
qqline(resid(model), col = "dodgerblue", lwd = 2)

shapiro.test(resid(model))
bptest(model)


#finding influenctial
mod_cook = cooks.distance(model)
n=length(resid(model))
high_infl = mod_cook > 4 / n
sum(high_infl)     
mean(high_infl)

#Refit the multiple regression model without any influential points
formula=as.formula(as.character(model$call[2]))
model_sub = lm(formula, data = autos_train, subset = !high_infl)
par(mfrow=c(1,2))
plot(fitted(model_sub), resid(model_sub), col = "dodgerblue", 
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(model_sub), main = "Normal Q-Q Plot", col = "dodgerblue")
qqline(resid(model_sub), col = "dodgerblue", lwd = 2)

shapiro.test(resid(model_sub))
bptest(model_sub)

summary(model_sub)$adj.r.squared
coef(summary(model_sub))

calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}


```


## test results

```{r}


```






TEST result?

LOOC_RMSE
CV 














# 3. Results

# 4. Discussion

# 5. Appendix




